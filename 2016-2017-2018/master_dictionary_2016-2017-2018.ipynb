{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "522c7692",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 264\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m--> 264\u001b[0m     base_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloughran_mcdonald_dictionary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m     LOG_FILE \u001b[38;5;241m=\u001b[39m base_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoad_MD_Logfile.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m     start \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Routine to load master dicitonary\n",
    "\n",
    "Bill McDonald\n",
    "Date: 201510 Updated: 202201 / 202308 / 202402\n",
    "\n",
    "----------------------------\n",
    "\n",
    "Retrieved by: Michael Fryer\n",
    "Retrieved from: https://sraf.nd.edu/loughranmcdonald-master-dictionary/\n",
    "Retrieved on: 2-12-2024\n",
    "\"\"\"\n",
    "\n",
    "import datetime as dt\n",
    "import sys\n",
    "\n",
    "\n",
    "def load_masterdictionary(file_path, print_flag=False, f_log=None, get_other=False):\n",
    "    start_local = dt.datetime.now()\n",
    "    # Setup dictionaries\n",
    "    _master_dictionary = {}\n",
    "    _sentiment_categories = [\n",
    "        \"negative\",\n",
    "        \"positive\",\n",
    "        \"uncertainty\",\n",
    "        \"litigious\",\n",
    "        \"strong_modal\",\n",
    "        \"weak_modal\",\n",
    "        \"constraining\",\n",
    "        \"complexity\",\n",
    "    ]\n",
    "    _sentiment_dictionaries = dict()\n",
    "    for sentiment in _sentiment_categories:\n",
    "        _sentiment_dictionaries[sentiment] = dict()\n",
    "\n",
    "    # Load slightly modified common stopwords.\n",
    "    # Dropped from traditional: A, I, S, T, DON, WILL, AGAINST\n",
    "    # Added: AMONG\n",
    "    _stopwords = [\n",
    "        \"ME\",\n",
    "        \"MY\",\n",
    "        \"MYSELF\",\n",
    "        \"WE\",\n",
    "        \"OUR\",\n",
    "        \"OURS\",\n",
    "        \"OURSELVES\",\n",
    "        \"YOU\",\n",
    "        \"YOUR\",\n",
    "        \"YOURS\",\n",
    "        \"YOURSELF\",\n",
    "        \"YOURSELVES\",\n",
    "        \"HE\",\n",
    "        \"HIM\",\n",
    "        \"HIS\",\n",
    "        \"HIMSELF\",\n",
    "        \"SHE\",\n",
    "        \"HER\",\n",
    "        \"HERS\",\n",
    "        \"HERSELF\",\n",
    "        \"IT\",\n",
    "        \"ITS\",\n",
    "        \"ITSELF\",\n",
    "        \"THEY\",\n",
    "        \"THEM\",\n",
    "        \"THEIR\",\n",
    "        \"THEIRS\",\n",
    "        \"THEMSELVES\",\n",
    "        \"WHAT\",\n",
    "        \"WHICH\",\n",
    "        \"WHO\",\n",
    "        \"WHOM\",\n",
    "        \"THIS\",\n",
    "        \"THAT\",\n",
    "        \"THESE\",\n",
    "        \"THOSE\",\n",
    "        \"AM\",\n",
    "        \"IS\",\n",
    "        \"ARE\",\n",
    "        \"WAS\",\n",
    "        \"WERE\",\n",
    "        \"BE\",\n",
    "        \"BEEN\",\n",
    "        \"BEING\",\n",
    "        \"HAVE\",\n",
    "        \"HAS\",\n",
    "        \"HAD\",\n",
    "        \"HAVING\",\n",
    "        \"DO\",\n",
    "        \"DOES\",\n",
    "        \"DID\",\n",
    "        \"DOING\",\n",
    "        \"AN\",\n",
    "        \"THE\",\n",
    "        \"AND\",\n",
    "        \"BUT\",\n",
    "        \"IF\",\n",
    "        \"OR\",\n",
    "        \"BECAUSE\",\n",
    "        \"AS\",\n",
    "        \"UNTIL\",\n",
    "        \"WHILE\",\n",
    "        \"OF\",\n",
    "        \"AT\",\n",
    "        \"BY\",\n",
    "        \"FOR\",\n",
    "        \"WITH\",\n",
    "        \"ABOUT\",\n",
    "        \"BETWEEN\",\n",
    "        \"INTO\",\n",
    "        \"THROUGH\",\n",
    "        \"DURING\",\n",
    "        \"BEFORE\",\n",
    "        \"AFTER\",\n",
    "        \"ABOVE\",\n",
    "        \"BELOW\",\n",
    "        \"TO\",\n",
    "        \"FROM\",\n",
    "        \"UP\",\n",
    "        \"DOWN\",\n",
    "        \"IN\",\n",
    "        \"OUT\",\n",
    "        \"ON\",\n",
    "        \"OFF\",\n",
    "        \"OVER\",\n",
    "        \"UNDER\",\n",
    "        \"AGAIN\",\n",
    "        \"FURTHER\",\n",
    "        \"THEN\",\n",
    "        \"ONCE\",\n",
    "        \"HERE\",\n",
    "        \"THERE\",\n",
    "        \"WHEN\",\n",
    "        \"WHERE\",\n",
    "        \"WHY\",\n",
    "        \"HOW\",\n",
    "        \"ALL\",\n",
    "        \"ANY\",\n",
    "        \"BOTH\",\n",
    "        \"EACH\",\n",
    "        \"FEW\",\n",
    "        \"MORE\",\n",
    "        \"MOST\",\n",
    "        \"OTHER\",\n",
    "        \"SOME\",\n",
    "        \"SUCH\",\n",
    "        \"NO\",\n",
    "        \"NOR\",\n",
    "        \"NOT\",\n",
    "        \"ONLY\",\n",
    "        \"OWN\",\n",
    "        \"SAME\",\n",
    "        \"SO\",\n",
    "        \"THAN\",\n",
    "        \"TOO\",\n",
    "        \"VERY\",\n",
    "        \"CAN\",\n",
    "        \"JUST\",\n",
    "        \"SHOULD\",\n",
    "        \"NOW\",\n",
    "        \"AMONG\",\n",
    "    ]\n",
    "\n",
    "    # Loop thru words and load dictionaries\n",
    "    with open(file_path) as f:\n",
    "        _total_documents = 0\n",
    "        _md_header = f.readline()  # Consume header line\n",
    "\n",
    "        for line in f:\n",
    "            cols = line.rstrip(\"\\n\").split(\",\")\n",
    "            word = cols[0]\n",
    "            _master_dictionary[word] = MasterDictionary(cols, _stopwords)\n",
    "            for sentiment in _sentiment_categories:\n",
    "                if getattr(_master_dictionary[word], sentiment):\n",
    "                    _sentiment_dictionaries[sentiment][word] = 0\n",
    "            _total_documents += _master_dictionary[cols[0]].doc_count\n",
    "            if len(_master_dictionary) % 5000 == 0 and print_flag:\n",
    "                print(\n",
    "                    f\"\\r ...Loading Master Dictionary {len(_master_dictionary):,}\",\n",
    "                    end=\"\",\n",
    "                    flush=True,\n",
    "                )\n",
    "\n",
    "    if print_flag:\n",
    "        print(\"\\r\", end=\"\")  # clear line\n",
    "        print(f\"\\nMaster Dictionary loaded from file:\\n  {file_path}\\n\")\n",
    "        print(f\"  master_dictionary has {len(_master_dictionary):,} words.\\n\")\n",
    "\n",
    "    if f_log:\n",
    "        try:\n",
    "            f_log.write(\n",
    "                \"\\n\\n  FUNCTION: load_masterdictionary\"\n",
    "                + \"(file_path, print_flag, f_log, get_other)\\n\"\n",
    "            )\n",
    "            f_log.write(f\"\\n    file_path  = {file_path}\")\n",
    "            f_log.write(f\"\\n    print_flag = {print_flag}\")\n",
    "            f_log.write(f\"\\n    f_log      = {f_log.name}\")\n",
    "            f_log.write(f\"\\n    get_other  = {get_other}\")\n",
    "            f_log.write(\n",
    "                f\"\\n\\n    {len(_master_dictionary):,} words loaded in master_dictionary.\\n\"\n",
    "            )\n",
    "            f_log.write(\"\\n    Sentiment:\")\n",
    "            for sentiment in _sentiment_categories:\n",
    "                f_log.write(\n",
    "                    f\"\\n      {sentiment:13}: {len(_sentiment_dictionaries[sentiment]):8,}\"\n",
    "                )\n",
    "            f_log.write(\n",
    "                f\"\\n\\n  END FUNCTION: load_masterdictionary: {(dt.datetime.now()-start_local)}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Log file in load_masterdictionary is not available for writing\")\n",
    "            print(f\"Error = {e}\")\n",
    "\n",
    "    if get_other:\n",
    "        return (\n",
    "            _master_dictionary,\n",
    "            _md_header,\n",
    "            _sentiment_categories,\n",
    "            _sentiment_dictionaries,\n",
    "            _stopwords,\n",
    "            _total_documents,\n",
    "        )\n",
    "    else:\n",
    "        return _master_dictionary\n",
    "\n",
    "\n",
    "class MasterDictionary:\n",
    "    def __init__(self, cols, _stopwords):\n",
    "        for ptr, col in enumerate(cols):\n",
    "            if col == \"\":\n",
    "                cols[ptr] = \"0\"\n",
    "        try:\n",
    "            self.word = cols[0].upper()\n",
    "            self.sequence_number = int(cols[1])\n",
    "            self.word_count = int(cols[2])\n",
    "            self.word_proportion = float(cols[3])\n",
    "            self.average_proportion = float(cols[4])\n",
    "            self.std_dev_prop = float(cols[5])\n",
    "            self.doc_count = int(cols[6])\n",
    "            self.negative = int(cols[7])\n",
    "            self.positive = int(cols[8])\n",
    "            self.uncertainty = int(cols[9])\n",
    "            self.litigious = int(cols[10])\n",
    "            self.strong_modal = int(cols[11])\n",
    "            self.weak_modal = int(cols[12])\n",
    "            self.constraining = int(cols[13])\n",
    "            self.complexity = int(cols[14])\n",
    "            self.syllables = int(cols[15])\n",
    "            self.source = cols[16]\n",
    "            if self.word in _stopwords:\n",
    "                self.stopword = True\n",
    "            else:\n",
    "                self.stopword = False\n",
    "        except Exception as e:\n",
    "            print(\"ERROR in class MasterDictionary\")\n",
    "            print(f\"word = {cols[0]} : seqnum = {cols[1]}\")\n",
    "            print(f\"Exception: {e}\")\n",
    "            quit()\n",
    "        return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "\n",
    "    base_path = Path(__file__).parent / \"loughran_mcdonald_dictionary\"\n",
    "    LOG_FILE = base_path / \"Load_MD_Logfile.txt\"\n",
    "    start = dt.datetime.now()\n",
    "    print(f'\\n\\n{start.strftime(\"%c\")}\\nPROGRAM NAME: {sys.argv[0]}\\n')\n",
    "    with open(LOG_FILE, \"w\", encoding=\"utf-8\") as f_log:\n",
    "        md = base_path / \"Loughran-McDonald_MasterDictionary_1993-2024.csv\"\n",
    "        (\n",
    "            master_dictionary,\n",
    "            md_header,\n",
    "            sentiment_categories,\n",
    "            sentiment_dictionaries,\n",
    "            stopwords,\n",
    "            total_documents,\n",
    "        ) = load_masterdictionary(md, True, f_log, True)\n",
    "        print(f\"\\n\\nRuntime: {(dt.datetime.now()-start)}\")\n",
    "        print(f'\\nNormal termination.\\n{dt.datetime.now().strftime(\"%c\")}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97510a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b0a94bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd()\n",
    "PROCESSED_DIR = BASE_DIR / \"processed\"\n",
    "RESULTS_FILE = BASE_DIR / \"sentiment_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cf84e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_DICT_FILE = BASE_DIR / \"Loughran-McDonald_MasterDictionary_1993-2021.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c98cb14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Thu Dec  4 16:22:35 2025\n",
      "PROGRAM NAME: /opt/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\n",
      "\n",
      " ...Loading Master Dictionary 85,000\n",
      "Master Dictionary loaded from file:\n",
      "  /Users/min/loughran_mcdonald_dictionary/Loughran-McDonald_MasterDictionary_1993-2024.csv\n",
      "\n",
      "  master_dictionary has 86,553 words.\n",
      "\n",
      "\n",
      "\n",
      "Runtime: 0:00:00.856693\n",
      "\n",
      "Normal termination.\n",
      "Thu Dec  4 16:22:36 2025\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Routine to load master dicitonary\n",
    "\n",
    "Bill McDonald\n",
    "Date: 201510 Updated: 202201 / 202308 / 202402\n",
    "\n",
    "----------------------------\n",
    "\n",
    "Retrieved by: Michael Fryer\n",
    "Retrieved from: https://sraf.nd.edu/loughranmcdonald-master-dictionary/\n",
    "Retrieved on: 2-12-2024\n",
    "\"\"\"\n",
    "\n",
    "import datetime as dt\n",
    "import sys\n",
    "\n",
    "\n",
    "def load_masterdictionary(file_path, print_flag=False, f_log=None, get_other=False):\n",
    "    start_local = dt.datetime.now()\n",
    "    # Setup dictionaries\n",
    "    _master_dictionary = {}\n",
    "    _sentiment_categories = [\n",
    "        \"negative\",\n",
    "        \"positive\",\n",
    "        \"uncertainty\",\n",
    "        \"litigious\",\n",
    "        \"strong_modal\",\n",
    "        \"weak_modal\",\n",
    "        \"constraining\",\n",
    "        \"complexity\",\n",
    "    ]\n",
    "    _sentiment_dictionaries = dict()\n",
    "    for sentiment in _sentiment_categories:\n",
    "        _sentiment_dictionaries[sentiment] = dict()\n",
    "\n",
    "    # Load slightly modified common stopwords.\n",
    "    # Dropped from traditional: A, I, S, T, DON, WILL, AGAINST\n",
    "    # Added: AMONG\n",
    "    _stopwords = [\n",
    "        \"ME\",\n",
    "        \"MY\",\n",
    "        \"MYSELF\",\n",
    "        \"WE\",\n",
    "        \"OUR\",\n",
    "        \"OURS\",\n",
    "        \"OURSELVES\",\n",
    "        \"YOU\",\n",
    "        \"YOUR\",\n",
    "        \"YOURS\",\n",
    "        \"YOURSELF\",\n",
    "        \"YOURSELVES\",\n",
    "        \"HE\",\n",
    "        \"HIM\",\n",
    "        \"HIS\",\n",
    "        \"HIMSELF\",\n",
    "        \"SHE\",\n",
    "        \"HER\",\n",
    "        \"HERS\",\n",
    "        \"HERSELF\",\n",
    "        \"IT\",\n",
    "        \"ITS\",\n",
    "        \"ITSELF\",\n",
    "        \"THEY\",\n",
    "        \"THEM\",\n",
    "        \"THEIR\",\n",
    "        \"THEIRS\",\n",
    "        \"THEMSELVES\",\n",
    "        \"WHAT\",\n",
    "        \"WHICH\",\n",
    "        \"WHO\",\n",
    "        \"WHOM\",\n",
    "        \"THIS\",\n",
    "        \"THAT\",\n",
    "        \"THESE\",\n",
    "        \"THOSE\",\n",
    "        \"AM\",\n",
    "        \"IS\",\n",
    "        \"ARE\",\n",
    "        \"WAS\",\n",
    "        \"WERE\",\n",
    "        \"BE\",\n",
    "        \"BEEN\",\n",
    "        \"BEING\",\n",
    "        \"HAVE\",\n",
    "        \"HAS\",\n",
    "        \"HAD\",\n",
    "        \"HAVING\",\n",
    "        \"DO\",\n",
    "        \"DOES\",\n",
    "        \"DID\",\n",
    "        \"DOING\",\n",
    "        \"AN\",\n",
    "        \"THE\",\n",
    "        \"AND\",\n",
    "        \"BUT\",\n",
    "        \"IF\",\n",
    "        \"OR\",\n",
    "        \"BECAUSE\",\n",
    "        \"AS\",\n",
    "        \"UNTIL\",\n",
    "        \"WHILE\",\n",
    "        \"OF\",\n",
    "        \"AT\",\n",
    "        \"BY\",\n",
    "        \"FOR\",\n",
    "        \"WITH\",\n",
    "        \"ABOUT\",\n",
    "        \"BETWEEN\",\n",
    "        \"INTO\",\n",
    "        \"THROUGH\",\n",
    "        \"DURING\",\n",
    "        \"BEFORE\",\n",
    "        \"AFTER\",\n",
    "        \"ABOVE\",\n",
    "        \"BELOW\",\n",
    "        \"TO\",\n",
    "        \"FROM\",\n",
    "        \"UP\",\n",
    "        \"DOWN\",\n",
    "        \"IN\",\n",
    "        \"OUT\",\n",
    "        \"ON\",\n",
    "        \"OFF\",\n",
    "        \"OVER\",\n",
    "        \"UNDER\",\n",
    "        \"AGAIN\",\n",
    "        \"FURTHER\",\n",
    "        \"THEN\",\n",
    "        \"ONCE\",\n",
    "        \"HERE\",\n",
    "        \"THERE\",\n",
    "        \"WHEN\",\n",
    "        \"WHERE\",\n",
    "        \"WHY\",\n",
    "        \"HOW\",\n",
    "        \"ALL\",\n",
    "        \"ANY\",\n",
    "        \"BOTH\",\n",
    "        \"EACH\",\n",
    "        \"FEW\",\n",
    "        \"MORE\",\n",
    "        \"MOST\",\n",
    "        \"OTHER\",\n",
    "        \"SOME\",\n",
    "        \"SUCH\",\n",
    "        \"NO\",\n",
    "        \"NOR\",\n",
    "        \"NOT\",\n",
    "        \"ONLY\",\n",
    "        \"OWN\",\n",
    "        \"SAME\",\n",
    "        \"SO\",\n",
    "        \"THAN\",\n",
    "        \"TOO\",\n",
    "        \"VERY\",\n",
    "        \"CAN\",\n",
    "        \"JUST\",\n",
    "        \"SHOULD\",\n",
    "        \"NOW\",\n",
    "        \"AMONG\",\n",
    "    ]\n",
    "\n",
    "    # Loop thru words and load dictionaries\n",
    "    with open(file_path) as f:\n",
    "        _total_documents = 0\n",
    "        _md_header = f.readline()  # Consume header line\n",
    "\n",
    "        for line in f:\n",
    "            cols = line.rstrip(\"\\n\").split(\",\")\n",
    "            word = cols[0]\n",
    "            _master_dictionary[word] = MasterDictionary(cols, _stopwords)\n",
    "            for sentiment in _sentiment_categories:\n",
    "                if getattr(_master_dictionary[word], sentiment):\n",
    "                    _sentiment_dictionaries[sentiment][word] = 0\n",
    "            _total_documents += _master_dictionary[cols[0]].doc_count\n",
    "            if len(_master_dictionary) % 5000 == 0 and print_flag:\n",
    "                print(\n",
    "                    f\"\\r ...Loading Master Dictionary {len(_master_dictionary):,}\",\n",
    "                    end=\"\",\n",
    "                    flush=True,\n",
    "                )\n",
    "\n",
    "    if print_flag:\n",
    "        print(\"\\r\", end=\"\")  # clear line\n",
    "        print(f\"\\nMaster Dictionary loaded from file:\\n  {file_path}\\n\")\n",
    "        print(f\"  master_dictionary has {len(_master_dictionary):,} words.\\n\")\n",
    "\n",
    "    if f_log:\n",
    "        try:\n",
    "            f_log.write(\n",
    "                \"\\n\\n  FUNCTION: load_masterdictionary\"\n",
    "                + \"(file_path, print_flag, f_log, get_other)\\n\"\n",
    "            )\n",
    "            f_log.write(f\"\\n    file_path  = {file_path}\")\n",
    "            f_log.write(f\"\\n    print_flag = {print_flag}\")\n",
    "            f_log.write(f\"\\n    f_log      = {f_log.name}\")\n",
    "            f_log.write(f\"\\n    get_other  = {get_other}\")\n",
    "            f_log.write(\n",
    "                f\"\\n\\n    {len(_master_dictionary):,} words loaded in master_dictionary.\\n\"\n",
    "            )\n",
    "            f_log.write(\"\\n    Sentiment:\")\n",
    "            for sentiment in _sentiment_categories:\n",
    "                f_log.write(\n",
    "                    f\"\\n      {sentiment:13}: {len(_sentiment_dictionaries[sentiment]):8,}\"\n",
    "                )\n",
    "            f_log.write(\n",
    "                f\"\\n\\n  END FUNCTION: load_masterdictionary: {(dt.datetime.now()-start_local)}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Log file in load_masterdictionary is not available for writing\")\n",
    "            print(f\"Error = {e}\")\n",
    "\n",
    "    if get_other:\n",
    "        return (\n",
    "            _master_dictionary,\n",
    "            _md_header,\n",
    "            _sentiment_categories,\n",
    "            _sentiment_dictionaries,\n",
    "            _stopwords,\n",
    "            _total_documents,\n",
    "        )\n",
    "    else:\n",
    "        return _master_dictionary\n",
    "\n",
    "\n",
    "class MasterDictionary:\n",
    "    def __init__(self, cols, _stopwords):\n",
    "        for ptr, col in enumerate(cols):\n",
    "            if col == \"\":\n",
    "                cols[ptr] = \"0\"\n",
    "        try:\n",
    "            self.word = cols[0].upper()\n",
    "            self.sequence_number = int(cols[1])\n",
    "            self.word_count = int(cols[2])\n",
    "            self.word_proportion = float(cols[3])\n",
    "            self.average_proportion = float(cols[4])\n",
    "            self.std_dev_prop = float(cols[5])\n",
    "            self.doc_count = int(cols[6])\n",
    "            self.negative = int(cols[7])\n",
    "            self.positive = int(cols[8])\n",
    "            self.uncertainty = int(cols[9])\n",
    "            self.litigious = int(cols[10])\n",
    "            self.strong_modal = int(cols[11])\n",
    "            self.weak_modal = int(cols[12])\n",
    "            self.constraining = int(cols[13])\n",
    "            self.complexity = int(cols[14])\n",
    "            self.syllables = int(cols[15])\n",
    "            self.source = cols[16]\n",
    "            if self.word in _stopwords:\n",
    "                self.stopword = True\n",
    "            else:\n",
    "                self.stopword = False\n",
    "        except Exception as e:\n",
    "            print(\"ERROR in class MasterDictionary\")\n",
    "            print(f\"word = {cols[0]} : seqnum = {cols[1]}\")\n",
    "            print(f\"Exception: {e}\")\n",
    "            quit()\n",
    "        return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    import datetime as dt\n",
    "\n",
    "    # Use Path.cwd() instead of Path(__file__).parent for Jupyter Notebook compatibility\n",
    "    base_path = Path.cwd() / \"loughran_mcdonald_dictionary\"\n",
    "    LOG_FILE = base_path / \"Load_MD_Logfile.txt\"\n",
    "    start = dt.datetime.now()\n",
    "    \n",
    "    print(f'\\n\\n{start.strftime(\"%c\")}\\nPROGRAM NAME: {sys.argv[0]}\\n')\n",
    "    with open(LOG_FILE, \"w\", encoding=\"utf-8\") as f_log:\n",
    "        md = base_path / \"Loughran-McDonald_MasterDictionary_1993-2024.csv\"\n",
    "        (\n",
    "            master_dictionary,\n",
    "            md_header,\n",
    "            sentiment_categories,\n",
    "            sentiment_dictionaries,\n",
    "            stopwords,\n",
    "            total_documents,\n",
    "        ) = load_masterdictionary(md, True, f_log, True)\n",
    "        print(f\"\\n\\nRuntime: {(dt.datetime.now()-start)}\")\n",
    "        print(f'\\nNormal termination.\\n{dt.datetime.now().strftime(\"%c\")}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9beeaee",
   "metadata": {},
   "source": [
    "**Read year 2016, 2017, 2018**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16d0777d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionary from: /Users/min/loughran_mcdonald_dictionary/Loughran-McDonald_MasterDictionary_1993-2024.csv\n",
      "Dictionary loaded. Starting analysis on 146 specified files...\n",
      "--------------------------------------------------\n",
      "Analysis Complete.\n",
      "Processed: 146 files.\n",
      "Results saved to: /Users/min/sentiment_analysis_results_189.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "BASE_DIR = Path.cwd()\n",
    "PROCESSED_DIR = BASE_DIR / \"processed\"\n",
    "OUTPUT_FILE = BASE_DIR / \"sentiment_analysis_results_189.csv\"\n",
    "DICT_PATH = Path(\"/Users/min/loughran_mcdonald_dictionary/Loughran-McDonald_MasterDictionary_1993-2024.csv\")\n",
    "\n",
    "# Month mapping for filename conversion\n",
    "MONTH_MAP = {\n",
    "    \"Jan\": \"01\", \"Feb\": \"02\", \"Mar\": \"03\", \"Apr\": \"04\", \"May\": \"05\", \"Jun\": \"06\",\n",
    "    \"Jul\": \"07\", \"Aug\": \"08\", \"Sep\": \"09\", \"Oct\": \"10\", \"Nov\": \"11\", \"Dec\": \"12\"\n",
    "}\n",
    "\n",
    "# The list of 189 specific files to analyze\n",
    "TARGET_RAW_FILES = [\n",
    "    # Tesco\n",
    "    \"2016-Apr-14-TSCO.L-139810984905-Transcript.txt\", \"2016-Apr-13-TSCO.L-137046574372-Transcript.txt\",\n",
    "    \"2016-Oct-05-TSCO.L-139932963486-Transcript.txt\", \"2016-Oct-05-TSCO.L-138488445184-Transcript.txt\",\n",
    "    \"2017-Apr-12-TSCO.L-140153460189-Transcript.txt\", \"2017-Oct-04-TSCO.L-138456938442-Transcript.txt\",\n",
    "    \"2017-Oct-04-TSCO.L-139408526789-Transcript.txt\", \"2018-Apr-11-TSCO.L-138743165366-Transcript.txt\",\n",
    "    \"2018-Apr-11-TSCO.L-139959883222-Transcript.txt\", \"2018-Oct-03-TSCO.L-139272056987-Transcript.txt\",\n",
    "    # Barclays\n",
    "    \"2016-Mar-01-BARC.L-139824873187-Transcript.txt\", \"2016-Mar-01-BARC.L-137548274670-Transcript.txt\",\n",
    "    \"2016-Jul-29-BARC.L-138523614655-Transcript.txt\", \"2016-Jul-29-BARC.L-139004361518-Transcript.txt\",\n",
    "    \"2017-Feb-23-BARC.L-139508609743-Transcript.txt\", \"2017-Feb-23-BARC.L-138555219749-Transcript.txt\",\n",
    "    \"2017-Jul-28-BARC.L-141010579615-Transcript.txt\", \"2017-Jul-28-BARC.L-138477306147-Transcript.txt\",\n",
    "    \"2017-Oct-26-BARC.L-137754650048-Transcript.txt\", \"2018-Feb-22-BARC.L-136963783161-Transcript.txt\",\n",
    "    \"2018-Feb-22-BARC.L-137901359180-Transcript.txt\", \"2018-Apr-26-BARC.L-139732542527-Transcript.txt\",\n",
    "    \"2018-Aug-02-BARC.L-139224686385-Transcript.txt\", \"2018-Aug-02-BARC.L-137366822025-Transcript.txt\",\n",
    "    \"2018-Oct-24-BARC.L-140944839383-Transcript.txt\",\n",
    "    # AstraZeneca\n",
    "    \"2016-Feb-04-AZN.L-137149101673-Transcript.txt\", \"2016-Apr-29-AZN.L-138460076277-Transcript.txt\",\n",
    "    \"2016-Jul-28-AZN.L-140055454105-Transcript.txt\", \"2016-Nov-10-AZN.L-140407419257-Transcript.txt\",\n",
    "    \"2017-Feb-02-AZN.L-140607754798-Transcript.txt\", \"2017-Apr-27-AZN.L-139861839552-Transcript.txt\",\n",
    "    \"2017-Jul-27-AZN.L-137003893218-Transcript.txt\", \"2017-Nov-09-AZN.L-138610867690-Transcript.txt\",\n",
    "    \"2018-Feb-02-AZN.L-137054448061-Transcript.txt\", \"2018-May-18-AZN.L-138645788490-Transcript.txt\",\n",
    "    \"2018-Jul-26-AZN.L-140818664492-Transcript.txt\", \"2018-Nov-08-AZN.L-140470213539-Transcript.txt\",\n",
    "    # Kroger\n",
    "    \"2016-Mar-03-KR.N-137071463801-Transcript.txt\", \"2016-Jun-16-KR.N-137382711624-Transcript.txt\",\n",
    "    \"2016-Sep-09-KR.N-139724544695-Transcript.txt\", \"2016-Dec-01-KR.N-138964739188-Transcript.txt\",\n",
    "    \"2017-Mar-02-KR.N-137268985869-Transcript.txt\", \"2017-Jun-15-KR.N-137028455557-Transcript.txt\",\n",
    "    \"2017-Sep-08-KR.N-139332442878-Transcript.txt\", \"2017-Nov-30-KR.N-139233726377-Transcript.txt\",\n",
    "    \"2018-Mar-08-KR.N-138743414845-Transcript.txt\", \"2018-Jun-21-KR.N-139399065775-Transcript.txt\",\n",
    "    \"2018-Sep-13-KR.N-140662712213-Transcript.txt\", \"2018-Dec-06-KR.N-137368561785-Transcript.txt\",\n",
    "    # Ford\n",
    "    \"2016-Jan-28-F.N-140702560456-Transcript.txt\", \"2016-Jan-28-F.N-140929213537-Transcript.txt\",\n",
    "    \"2016-Apr-28-F.N-139397969691-Transcript.txt\", \"2016-Apr-28-F.N-139061241116-Transcript.txt\",\n",
    "    \"2016-Jul-28-F.N-140686794229-Transcript.txt\", \"2016-Jul-28-F.N-138385069851-Transcript.txt\",\n",
    "    \"2016-Oct-27-F.N-139351278755-Transcript.txt\", \"2016-Oct-27-F.N-138379355375-Transcript.txt\",\n",
    "    \"2017-Jan-26-F.N-139159806156-Transcript.txt\", \"2017-Jan-26-F.N-141181459722-Transcript.txt\",\n",
    "    \"2017-Apr-27-F.N-140902625025-Transcript.txt\", \"2017-Jul-26-F.N-138852696142-Transcript.txt\",\n",
    "    \"2017-Oct-26-F.N-137075183639-Transcript.txt\", \"2018-Jan-24-F.N-138389904781-Transcript.txt\",\n",
    "    \"2018-Apr-25-F.N-137616469531-Transcript.txt\", \"2018-Jul-25-F.N-140890935247-Transcript.txt\",\n",
    "    \"2018-Oct-24-F.N-137531908921-Transcript.txt\",\n",
    "    # Johnson & Johnson\n",
    "    \"2016-Jan-26-JNJ.N-138676405818-Transcript.txt\", \"2016-Apr-19-JNJ.N-137441522591-Transcript.txt\",\n",
    "    \"2016-Jul-19-JNJ.N-140074050620-Transcript.txt\", \"2016-Oct-18-JNJ.N-139180104058-Transcript.txt\",\n",
    "    \"2017-Jan-24-JNJ.N-137993682362-Transcript.txt\", \"2017-Apr-18-JNJ.N-139863288812-Transcript.txt\",\n",
    "    \"2017-Jul-18-JNJ.N-137992811209-Transcript.txt\", \"2017-Oct-17-JNJ.N-137387649725-Transcript.txt\",\n",
    "    \"2018-Jan-23-JNJ.N-137596207567-Transcript.txt\", \"2018-Apr-17-JNJ.N-137267325560-Transcript.txt\",\n",
    "    \"2018-Jul-17-JNJ.N-141010222562-Transcript.txt\", \"2018-Oct-16-JNJ.N-138373365491-Transcript.txt\",\n",
    "    # Kraft Heinz\n",
    "    \"2016-Feb-25-KHC.OQ-138842966670-Transcript.txt\", \"2016-May-04-KHC.OQ-139041781128-Transcript.txt\",\n",
    "    \"2016-Aug-04-KHC.OQ-138495811214-Transcript.txt\", \"2016-Nov-03-KHC.OQ-137203242866-Transcript.txt\",\n",
    "    \"2017-Feb-15-KHC.OQ-139150056386-Transcript.txt\", \"2017-May-03-KHC.OQ-139224781261-Transcript.txt\",\n",
    "    \"2017-Aug-03-KHC.OQ-140749085059-Transcript.txt\", \"2017-Nov-01-KHC.OQ-138744889212-Transcript.txt\",\n",
    "    \"2018-Feb-16-KHC.OQ-137796859188-Transcript.txt\", \"2018-May-02-KHC.OQ-140336666357-Transcript.txt\",\n",
    "    \"2018-Aug-03-KHC.OQ-139625297076-Transcript.txt\", \"2018-Nov-01-KHC.OQ-139957170311-Transcript.txt\",\n",
    "    # DHL\n",
    "    \"2016-Mar-09-DPWGn.DE-137982971470-Transcript.txt\", \"2016-May-11-DPWGn.DE-141134862749-Transcript.txt\",\n",
    "    \"2016-Aug-03-DPWGn.DE-140062875928-Transcript.txt\", \"2016-Nov-08-DPWGn.DE-138136081245-Transcript.txt\",\n",
    "    \"2017-Mar-08-DPWGn.DE-139183886140-Transcript.txt\", \"2017-May-11-DPWGn.DE-139196110732-Transcript.txt\",\n",
    "    \"2017-Aug-08-DPWGn.DE-139817232520-Transcript.txt\", \"2017-Nov-09-DPWGn.DE-136901510295-Transcript.txt\",\n",
    "    \"2018-Mar-07-DPWGn.DE-140776301443-Transcript.txt\", \"2018-Aug-07-DPWGn.DE-137436527491-Transcript.txt\",\n",
    "    \"2018-Nov-06-DPWGn.DE-138067543106-Transcript.txt\", \"2018-Nov-06-DPWGn.DE-139056129466-Transcript.txt\",\n",
    "    # Toyota\n",
    "    \"2016-Feb-05-7203.T-139386834162-Transcript.txt\", \"2016-May-11-7203.T-139232969728-Transcript.txt\",\n",
    "    \"2016-Aug-04-7203.T-140552523227-Transcript.txt\", \"2016-Nov-08-7203.T-140750745571-Transcript.txt\",\n",
    "    \"2017-Feb-06-7203.T-139401254994-Transcript.txt\", \"2017-May-10-7203.T-141025822521-Transcript.txt\",\n",
    "    \"2017-May-10-7203.T-139668947849-Transcript.txt\", \"2017-Aug-04-7203.T-140487691474-Transcript.txt\",\n",
    "    \"2017-Nov-07-7203.T-139563770726-Transcript.txt\", \"2018-Feb-06-7203.T-138825723624-Transcript.txt\",\n",
    "    \"2018-May-09-7203.T-137864330980-Transcript.txt\", \"2018-May-09-7203.T-139256922882-Transcript.txt\",\n",
    "    \"2018-Aug-03-7203.T-137294661410-Transcript.txt\", \"2018-Nov-05-7203.T-136945998670-Transcript.txt\",\n",
    "    # Eli Lilly\n",
    "    \"2016-Jan-28-LLY.N-139753177768-Transcript.txt\", \"2016-Apr-26-LLY.N-138440853820-Transcript.txt\",\n",
    "    \"2016-Jul-26-LLY.N-138043614001-Transcript.txt\", \"2016-Oct-25-LLY.N-137654586127-Transcript.txt\",\n",
    "    \"2017-Jan-31-LLY.N-137441172831-Transcript.txt\", \"2017-Apr-25-LLY.N-138399030694-Transcript.txt\",\n",
    "    \"2017-Jul-25-LLY.N-137807180604-Transcript.txt\", \"2017-Oct-24-LLY.N-139538448754-Transcript.txt\",\n",
    "    \"2018-Jan-31-LLY.N-138912850488-Transcript.txt\", \"2018-Apr-24-LLY.N-139809901587-Transcript.txt\",\n",
    "    \"2018-Jul-24-LLY.N-139253294349-Transcript.txt\", \"2018-Nov-06-LLY.N-138855989064-Transcript.txt\",\n",
    "    # Unilever\n",
    "    \"2016-Jan-19-ULVR.L-139914041816-Transcript.txt\", \"2016-Jul-21-ULVR.L-136975403569-Transcript.txt\",\n",
    "    \"2017-Jan-26-ULVR.L-137807207448-Transcript.txt\", \"2017-Jul-20-ULVR.L-140135791033-Transcript.txt\",\n",
    "    \"2018-Feb-01-ULVR.L-137010525657-Transcript.txt\", \"2018-Jul-19-ULVR.L-137927763788-Transcript.txt\",\n",
    "    # Land Rover (Tata)\n",
    "    \"2016-Feb-03-TATA.NS-137676422472-Transcript.txt\", \"2016-May-30-TATA.NS-138515578162-Transcript.txt\",\n",
    "    \"2016-Aug-02-TATA.NS-137290025993-Transcript.txt\", \"2016-Nov-09-TATA.NS-138685415110-Transcript.txt\",\n",
    "    \"2017-Jan-24-TATA.NS-139471832639-Transcript.txt\", \"2017-May-04-TATA.NS-140768671254-Transcript.txt\",\n",
    "    \"2017-Jul-25-TATA.NS-137873824680-Transcript.txt\", \"2017-Oct-26-TATA.NS-139209110121-Transcript.txt\",\n",
    "    \"2018-Feb-09-TATA.NS-139173494325-Transcript.txt\", \"2018-May-11-TATA.NS-137190394224-Transcript.txt\",\n",
    "    \"2018-Aug-10-TATA.NS-138219347027-Transcript.txt\", \"2018-Nov-02-TATA.NS-137854802714-Transcript.txt\"\n",
    "]\n",
    "\n",
    "# ================= CORE LOGIC =================\n",
    "\n",
    "def load_master_dictionary(file_path):\n",
    "    master_dict = {}\n",
    "    sentiment_categories = ['Negative', 'Positive', 'Uncertainty', 'Litigious', 'StrongModal', 'WeakModal', 'Constraining']\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"Error: Dictionary not found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Loading dictionary from: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            word = row['Word'].upper()\n",
    "            master_dict[word] = {}\n",
    "            for category in sentiment_categories:\n",
    "                try:\n",
    "                    if int(row[category]) > 0:\n",
    "                        master_dict[word][category] = True\n",
    "                except (ValueError, KeyError):\n",
    "                    continue\n",
    "    return master_dict\n",
    "\n",
    "def analyze_file(file_path, master_dict):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().upper()\n",
    "    \n",
    "    # Tokenize: Keep only uppercase words with length > 1\n",
    "    tokens = re.findall(r'\\b[A-Z]{2,}\\b', text)\n",
    "    \n",
    "    counts = {\n",
    "        'Negative': 0, 'Positive': 0, 'Uncertainty': 0, \n",
    "        'Litigious': 0, 'StrongModal': 0, 'WeakModal': 0, 'Constraining': 0\n",
    "    }\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in master_dict:\n",
    "            for cat in counts:\n",
    "                if cat in master_dict[token]:\n",
    "                    counts[cat] += 1\n",
    "                    \n",
    "    return len(tokens), counts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    master_dictionary = load_master_dictionary(DICT_PATH)\n",
    "    \n",
    "    if master_dictionary:\n",
    "        print(f\"Dictionary loaded. Starting analysis on {len(TARGET_RAW_FILES)} specified files...\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for raw_name in TARGET_RAW_FILES:\n",
    "            # Parse raw filename to construct the path in the 'processed' folder\n",
    "            # Raw: 2016-Apr-14-TSCO.L-139810984905-Transcript.txt\n",
    "            # Processed target: processed/2016/TSCO.L-2016-04-14.txt\n",
    "            \n",
    "            parts = raw_name.split(\"-\")\n",
    "            \n",
    "            if len(parts) >= 4:\n",
    "                year = parts[0]\n",
    "                month_str = parts[1]\n",
    "                day = parts[2]\n",
    "                ticker = parts[3]\n",
    "                \n",
    "                # Convert month string to number (e.g., Apr -> 04)\n",
    "                month_num = MONTH_MAP.get(month_str, \"00\")\n",
    "                \n",
    "                # Construct new filename and path\n",
    "                new_fname = f\"{ticker}-{year}-{month_num}-{day}.txt\"\n",
    "                file_path = PROCESSED_DIR / year / new_fname\n",
    "                \n",
    "                # Check existence and analyze\n",
    "                if file_path.exists():\n",
    "                    total_words, scores = analyze_file(file_path, master_dictionary)\n",
    "                    \n",
    "                    # Calculate Net Sentiment (Pos - Neg) / Total\n",
    "                    if total_words > 0:\n",
    "                        net_sentiment = (scores['Positive'] - scores['Negative']) / total_words\n",
    "                    else:\n",
    "                        net_sentiment = 0\n",
    "                    \n",
    "                    row = {\n",
    "                        'Filename': raw_name,\n",
    "                        'Ticker': ticker,\n",
    "                        'Date': f\"{year}-{month_num}-{day}\",\n",
    "                        'Total_Words': total_words,\n",
    "                        'Net_Sentiment': net_sentiment,\n",
    "                        **scores \n",
    "                    }\n",
    "                    results.append(row)\n",
    "                else:\n",
    "                    print(f\"Warning: File not found on disk: {file_path}\")\n",
    "            else:\n",
    "                 print(f\"Warning: Invalid filename format: {raw_name}\")\n",
    "\n",
    "        # Save to CSV\n",
    "        if results:\n",
    "            keys = results[0].keys()\n",
    "            with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=keys)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(results)\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "            print(\"Analysis Complete.\")\n",
    "            print(f\"Processed: {len(results)} files.\")\n",
    "            print(f\"Results saved to: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a7a7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
